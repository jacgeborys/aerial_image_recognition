{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["L4d0sKOF-mx-"],"gpuType":"T4","authorship_tag":"ABX9TyNbqwexeI7dJC3KXruojb4m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2MuynVH_w2Md","executionInfo":{"status":"ok","timestamp":1730321929749,"user_tz":-60,"elapsed":27410,"user":{"displayName":"Jacek Gęborys","userId":"15708106607875074455"}},"outputId":"d80e6aad-0ea4-467b-f25d-a91d633561c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"]},{"cell_type":"markdown","source":["# YOLO"],"metadata":{"id":"L4d0sKOF-mx-"}},{"cell_type":"code","source":["!pip install ultralytics"],"metadata":{"id":"U-etQx7MFYJ_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","import cv2\n","import requests\n","from osgeo import gdal\n","import geopandas as gpd\n","from shapely.geometry import Point, box\n","from tqdm import tqdm\n","from ultralytics import YOLO\n","from io import BytesIO\n","\n","# Load the trained YOLO model\n","checkpoint_path = '/content/drive/MyDrive/aerial_image_recognition/img/train/Tokyo/yolov8_tokyo_checkpoint.pt'\n","model = YOLO(checkpoint_path)\n","print(\"Model loaded from checkpoint successfully!\")\n","\n","# Load the 'ramki.shp' layer using Geopandas\n","ramki_shp_path = '/content/drive/MyDrive/aerial_image_recognition/gis/shp/ramki.shp'\n","ramki_gdf = gpd.read_file(ramki_shp_path)\n","\n","# Select the specific area named 'srodmiescie' (adjust this filter for your case)\n","srodmiescie_gdf = ramki_gdf[ramki_gdf['name'] == 'srodmiescie']\n","\n","# Get the bounding box of 'srodmiescie'\n","minx, miny, maxx, maxy = srodmiescie_gdf.total_bounds\n","print(f\"Bounding box of srodmiescie: {minx}, {miny}, {maxx}, {maxy}\")\n","\n","# Define WMS parameters\n","wms_url = \"https://mapy.geoportal.gov.pl/wss/service/PZGIK/ORTO/WMS/HighResolution\"\n","wms_params = {\n","    'service': 'WMS',\n","    'version': '1.3.0',\n","    'request': 'GetMap',\n","    'layers': 'ORTO',\n","    'styles': '',\n","    'crs': 'EPSG:3857',  # Match to your projected CRS (Web Mercator)\n","    'bbox': f'{minx},{miny},{maxx},{maxy}',  # Update for each tile\n","    'width': 1200,  # Set the image size matching your model\n","    'height': 1200,\n","    'format': 'image/tiff'\n","}\n","\n","# Define an empty GeoDataFrame for storing car/truck detections\n","car_centroids_gdf = gpd.GeoDataFrame(columns=['geometry', 'class', 'confidence'])\n","\n","# Loop through and fetch tiles from the WMS server\n","stride = 600  # Adjust stride for partial overlap\n","tile_size = 1200\n","\n","# Split the area into smaller tiles based on stride and tile size\n","x_coords = list(range(int(minx), int(maxx), stride))\n","y_coords = list(range(int(miny), int(maxy), stride))\n","\n","total_tiles = len(x_coords) * len(y_coords)\n","\n","with tqdm(total=total_tiles, desc=\"Processing Tiles\") as pbar:\n","    for x in x_coords:\n","        for y in y_coords:\n","            # Adjust bounding box for each tile\n","            tile_bbox = f'{x},{y},{x+tile_size},{y+tile_size}'\n","            wms_params['bbox'] = tile_bbox\n","\n","            # Request the tile from WMS\n","            response = requests.get(wms_url, params=wms_params)\n","\n","            if response.status_code == 200:\n","                # Load the image into OpenCV\n","                img_data = BytesIO(response.content)\n","                img = gdal.Open(img_data).ReadAsArray()\n","                img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n","\n","                # Run YOLO inference on the tile\n","                results = model(img)\n","\n","                # Extract bounding boxes and append to GeoDataFrame\n","                for box in results[0].boxes:\n","                    if box.cls in [0, 1] and box.conf > 0.4:  # Only consider cars/trucks\n","                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n","                        centroid_x = (x1 + x2) / 2 + x  # Adjust based on the tile's position\n","                        centroid_y = (y1 + y2) / 2 + y\n","\n","                        # Append the result to the GeoDataFrame\n","                        car_centroids_gdf = car_centroids_gdf.append({\n","                            'geometry': Point(centroid_x, centroid_y),\n","                            'class': 'Car' if box.cls == 0 else 'Truck',\n","                            'confidence': box.conf.item()\n","                        }, ignore_index=True)\n","\n","            pbar.update(1)\n","\n","# Set CRS to match the WMS (EPSG:3857 for Web Mercator)\n","car_centroids_gdf.set_crs(epsg=3857, inplace=True)\n","\n","# Save the results to a GeoJSON file\n","output_geojson_path = '/content/drive/MyDrive/aerial_image_recognition/gis/car_centroids_2.geojson'\n","car_centroids_gdf.to_file(output_geojson_path, driver='GeoJSON')\n","\n","print(f\"Car and truck centroids saved to {output_geojson_path}\")\n"],"metadata":{"id":"KBFmimBOw7bw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ONNX"],"metadata":{"id":"pq9CLeg_-j15"}},{"cell_type":"markdown","source":["## CPU"],"metadata":{"id":"ffhOazu4CYiQ"}},{"cell_type":"code","source":["!pip install onnxruntime-gpu\n","!pip install owslib\n","!pip install geopandas\n","!pip install pyproj\n","!pip install tqdm\n","!pip install dnspython"],"metadata":{"id":"0nt-Wnwb_QM_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Check PyTorch CUDA availability\n","import torch\n","print(\"CUDA available:\", torch.cuda.is_available())\n","if torch.cuda.is_available():\n","    print(\"CUDA device:\", torch.cuda.get_device_name(0))\n","    print(\"CUDA version:\", torch.version.cuda)\n","\n","# Check NVIDIA driver and CUDA version\n","!nvidia-smi\n","\n","# Check ONNX providers\n","import onnxruntime as ort\n","print(\"\\nAvailable ONNX providers:\", ort.get_available_providers())\n","print(\"Current provider options:\", ort.get_device())"],"metadata":{"id":"GDyR_YqxXoel"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnxruntime as ort\n","import numpy as np\n","import cv2\n","from owslib.wms import WebMapService\n","import geopandas as gpd\n","from shapely.geometry import Point, box\n","import io\n","from PIL import Image\n","import os\n","from google.colab import drive\n","from tqdm import tqdm\n","import time\n","import gc\n","import logging\n","import math\n","from pyproj import Transformer\n","from shapely.ops import transform\n","import pyproj\n","\n","# Configuration settings\n","CONFIG = {\n","    'wms_url': \"https://mapy.geoportal.gov.pl/wss/service/PZGIK/ORTO/WMS/StandardResolution\",\n","    'tile_size_meters': 64.0,      # 50m tiles\n","    'pixel_size': 0.10,          # 10cm/pixel\n","    'model_input_size': 640,     # Model input size\n","    'confidence_threshold': 0.4,  # Lower confidence threshold (was 0.5)\n","    'tile_overlap': 0.2,         # 20% overlap between tiles\n","    'duplicate_distance_threshold': 2e-5  # For removing duplicates from overlapping areas\n","}\n","\n","def setup_logging(base_dir):\n","    \"\"\"Minimal logging configuration - file only, no console output\"\"\"\n","    log_path = os.path.join(base_dir, 'detection_debug.log')\n","\n","    # Suppress all logging to console\n","    logging.getLogger().handlers = []\n","\n","    # File-only handler\n","    file_handler = logging.FileHandler(log_path, mode='w')\n","    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n","\n","    # Root logger setup\n","    root_logger = logging.getLogger()\n","    root_logger.addHandler(file_handler)\n","    root_logger.setLevel(logging.INFO)\n","\n","    # Suppress specific loggers that might be noisy\n","    logging.getLogger('owslib').setLevel(logging.WARNING)\n","    logging.getLogger('urllib3').setLevel(logging.WARNING)\n","\n","    return log_path\n","\n","def setup_environment():\n","    \"\"\"Setup paths and mount Google Drive\"\"\"\n","    drive.mount('/content/drive', force_remount=True)\n","    base_dir = \"/content/drive/Othercomputers/My_laptop/car_recognition\"\n","\n","    # Setup logging first\n","    log_path = setup_logging(base_dir)\n","\n","    return {\n","        'model_path': os.path.join(base_dir, \"models/car_aerial_detection_yolo7_ITCVD_deepness.onnx\"),\n","        'frame_path': os.path.join(base_dir, \"gis/shp/frames/warsaw.shp\"),\n","        'output_path': os.path.join(base_dir, 'gis/shp/detection_results/warsaw.geojson'),\n","        'tiles_path': os.path.join(base_dir, 'gis/shp/processing_tiles_warsaw.geojson'),\n","        'log_path': log_path\n","    }\n","\n","def meters_to_degrees(meters, latitude):\n","    \"\"\"Convert meters to degrees at a given latitude\"\"\"\n","    earth_radius = 6378137\n","    degrees_longitude = meters / (earth_radius * math.cos(math.radians(latitude)) * 2 * math.pi / 360)\n","    degrees_latitude = meters / (earth_radius * 2 * math.pi / 360)\n","    return degrees_longitude, degrees_latitude\n","\n","def get_tile_bboxes(bbox, tile_size_meters):\n","    \"\"\"Generate overlapping tile bounding boxes\"\"\"\n","    minx, miny, maxx, maxy = bbox\n","\n","    # Calculate tile size in degrees at the middle latitude\n","    mid_lat = (miny + maxy) / 2\n","    tile_size_lon, tile_size_lat = meters_to_degrees(tile_size_meters, mid_lat)\n","\n","    # Calculate overlap in degrees\n","    overlap_lon = tile_size_lon * CONFIG['tile_overlap']\n","    overlap_lat = tile_size_lat * CONFIG['tile_overlap']\n","\n","    # Calculate step size (tile size minus overlap)\n","    step_lon = tile_size_lon * (1 - CONFIG['tile_overlap'])\n","    step_lat = tile_size_lat * (1 - CONFIG['tile_overlap'])\n","\n","    tile_bboxes = []\n","    x = minx\n","    while x < maxx:\n","        y = miny\n","        while y < maxy:\n","            tile_bbox = (\n","                x,\n","                y,\n","                min(x + tile_size_lon, maxx),\n","                min(y + tile_size_lat, maxy)\n","            )\n","            tile_bboxes.append(tile_bbox)\n","            y += step_lat\n","        x += step_lon\n","\n","    return tile_bboxes\n","\n","def get_wms_image(wms, bbox):\n","    \"\"\"Simplified WMS image retrieval without logging\"\"\"\n","    width_pixels = height_pixels = CONFIG['model_input_size']\n","\n","    for attempt in range(3):\n","        try:\n","            img = wms.getmap(\n","                layers=['Raster'],\n","                srs='EPSG:4326',\n","                bbox=bbox,\n","                size=(width_pixels, height_pixels),\n","                format='image/png',\n","                transparent=True\n","            )\n","\n","            return Image.open(io.BytesIO(img.read())).convert('RGB')\n","\n","        except Exception as e:\n","            if attempt == 2:\n","                raise\n","            time.sleep(1)\n","\n","def analyze_model(session):\n","    \"\"\"Analyze ONNX model structure and outputs\"\"\"\n","    # Get model inputs\n","    inputs = session.get_inputs()\n","    outputs = session.get_outputs()\n","\n","    logging.info(\"\\nModel Analysis:\")\n","    logging.info(\"Input details:\")\n","    for input in inputs:\n","        logging.info(f\"Name: {input.name}\")\n","        logging.info(f\"Shape: {input.shape}\")\n","        logging.info(f\"Type: {input.type}\")\n","\n","    logging.info(\"\\nOutput details:\")\n","    for output in outputs:\n","        logging.info(f\"Name: {output.name}\")\n","        logging.info(f\"Shape: {output.shape}\")\n","        logging.info(f\"Type: {output.type}\")\n","\n","    # The number of classes can be inferred from output shape\n","    # YOLO output shape is typically [batch, num_boxes, num_classes + 5]\n","    num_classes = outputs[0].shape[2] - 5  # subtract 5 for box coords + confidence\n","    logging.info(f\"\\nNumber of classes: {num_classes}\")\n","\n","    return num_classes\n","\n","def process_detections(outputs, tile_bbox, num_classes):\n","    \"\"\"Process model outputs with class information\"\"\"\n","    detections = []\n","    boxes = outputs[0][0]\n","\n","    for box in boxes:\n","        # Get confidence and class scores\n","        confidence = box[4]\n","        if confidence > CONFIG['confidence_threshold']:\n","            # YOLO format: center_x, center_y, width, height, confidence, [class_scores]\n","            center_x, center_y, width, height = box[:4]\n","\n","            # Get class probabilities and best class\n","            class_scores = box[5:5+num_classes]\n","            class_id = np.argmax(class_scores)\n","            class_score = class_scores[class_id]\n","\n","            # Calculate geographic coordinates as before\n","            norm_x = center_x / CONFIG['model_input_size']\n","            norm_y = center_y / CONFIG['model_input_size']\n","\n","            longitude = tile_bbox[0] + (norm_x * (tile_bbox[2] - tile_bbox[0]))\n","            latitude = tile_bbox[3] - (norm_y * (tile_bbox[3] - tile_bbox[1]))\n","\n","            detections.append({\n","                'geometry': Point(longitude, latitude),\n","                'confidence': float(confidence),\n","                'class_id': int(class_id),\n","                'class_score': float(class_score),\n","                'bbox': [float(x) for x in box[:4]],\n","                'bbox_width': float(width),\n","                'bbox_height': float(height)\n","            })\n","\n","    return detections\n","\n","def improve_detection(img):\n","    \"\"\"Simplified image preprocessing with most effective enhancements\"\"\"\n","    # Only keep the most effective enhancements\n","    lab = cv2.cvtColor(img, cv2.COLOR_RGB2LAB)\n","    l, a, b = cv2.split(lab)\n","    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n","    l_enhanced = clahe.apply(l)\n","    enhanced_lab = cv2.merge((l_enhanced, a, b))\n","    enhanced = cv2.cvtColor(enhanced_lab, cv2.COLOR_LAB2RGB)\n","\n","    return [img, enhanced]\n","\n","def process_multiscale(wms, session, tile_bbox, num_classes, scales=[1.0]):\n","    \"\"\"Simplified multiscale processing with fewer scales\"\"\"\n","    all_detections = []\n","\n","    for scale in scales:\n","        try:\n","            original_size = CONFIG['tile_size_meters']\n","            scaled_size = original_size * scale\n","            center_lon = (tile_bbox[0] + tile_bbox[2]) / 2\n","            center_lat = (tile_bbox[1] + tile_bbox[3]) / 2\n","            size_lon, size_lat = meters_to_degrees(scaled_size, center_lat)\n","\n","            scaled_bbox = (\n","                center_lon - size_lon/2,\n","                center_lat - size_lat/2,\n","                center_lon + size_lon/2,\n","                center_lat + size_lat/2\n","            )\n","\n","            img = get_wms_image(wms, scaled_bbox)\n","            if img is None:\n","                continue\n","\n","            img_array = np.array(img)\n","            enhanced_images = improve_detection(img_array)\n","\n","            for enhanced in enhanced_images:\n","                input_tensor = cv2.cvtColor(enhanced, cv2.COLOR_RGB2BGR)\n","                input_tensor = np.transpose(input_tensor, (2, 0, 1))\n","                input_tensor = np.expand_dims(input_tensor, axis=0).astype(np.float32) / 255.0\n","\n","                outputs = session.run(None, {session.get_inputs()[0].name: input_tensor})\n","                detections = process_detections(outputs, scaled_bbox, num_classes)\n","\n","                for det in detections:\n","                    det['detection_scale'] = scale\n","                    all_detections.append(det)\n","\n","        except Exception as e:\n","            logging.error(f\"Error processing tile at scale {scale}: {str(e)}\")\n","            continue\n","\n","    return all_detections\n","\n","def remove_duplicates(detections_gdf):\n","    \"\"\"Duplicate removal with 2m threshold and progress bar\"\"\"\n","    if len(detections_gdf) == 0:\n","        return detections_gdf\n","\n","    # Create UTM transformer for accurate distances\n","    proj_utm = pyproj.CRS('EPSG:32634')  # UTM zone 34N\n","    proj_wgs = pyproj.CRS('EPSG:4326')\n","    project = pyproj.Transformer.from_crs(proj_wgs, proj_utm, always_xy=True).transform\n","\n","    # Convert to UTM\n","    detections_utm = detections_gdf.copy()\n","    detections_utm.geometry = detections_utm.geometry.apply(lambda geom: transform(project, geom))\n","\n","    # Sort by confidence\n","    detections_utm = detections_utm.sort_values('confidence', ascending=False)\n","\n","    kept_indices = []\n","    used_indices = set()\n","    distance_threshold = 2.0  # 2 meters\n","\n","    # Add progress bar\n","    with tqdm(total=len(detections_utm), desc=\"Removing duplicates\", leave=False) as pbar:\n","        for idx in detections_utm.index:\n","            if idx in used_indices:\n","                pbar.update(1)\n","                continue\n","\n","            kept_indices.append(idx)\n","            point = detections_utm.loc[idx, 'geometry']\n","\n","            # Find nearby points\n","            for other_idx in detections_utm.index:\n","                if other_idx != idx and other_idx not in used_indices:\n","                    other_point = detections_utm.loc[other_idx, 'geometry']\n","                    distance = point.distance(other_point)\n","\n","                    if distance < distance_threshold:\n","                        used_indices.add(other_idx)\n","\n","            pbar.update(1)\n","\n","    # Convert back to WGS84\n","    project_back = pyproj.Transformer.from_crs(proj_utm, proj_wgs, always_xy=True).transform\n","    filtered_gdf = detections_gdf.loc[kept_indices].copy()\n","    filtered_gdf.geometry = filtered_gdf.geometry.apply(lambda geom: transform(project_back, transform(project, geom)))\n","\n","    return filtered_gdf\n","\n","def main():\n","    \"\"\"Main execution with minimal console output\"\"\"\n","    try:\n","        paths = setup_environment()\n","        start_time = time.time()\n","\n","        # Initialize components silently\n","        wms = WebMapService(CONFIG['wms_url'], version='1.3.0')\n","        session = ort.InferenceSession(paths['model_path'])\n","        num_classes = analyze_model(session)\n","\n","        # Load and process frame\n","        frame_gdf = gpd.read_file(paths['frame_path'])\n","        if frame_gdf.crs.to_epsg() != 4326:\n","            frame_gdf = frame_gdf.to_crs(epsg=4326)\n","\n","        bbox = frame_gdf.total_bounds\n","        tile_bboxes = get_tile_bboxes(bbox, CONFIG['tile_size_meters'])\n","\n","        # Process tiles with clean progress bar\n","        all_detections = []\n","        with tqdm(total=len(tile_bboxes), desc=\"Processing tiles\", unit=\"tile\") as pbar:\n","            for tile_bbox in tile_bboxes:\n","                try:\n","                    detections = process_multiscale(wms, session, tile_bbox, num_classes, scales=[0.85, 1.0, 1.15])\n","                    if detections:\n","                        all_detections.extend(detections)\n","                except Exception as e:\n","                    # Silently log error to file and continue\n","                    logging.error(f\"Tile processing error: {str(e)}\")\n","                finally:\n","                    pbar.update(1)\n","                    gc.collect()\n","\n","        # Create and process detections DataFrame\n","        if not all_detections:\n","            detections_gdf = gpd.GeoDataFrame(\n","                columns=['geometry', 'confidence', 'class_id', 'class_score', 'bbox'],\n","                crs=\"EPSG:4326\"\n","            )\n","        else:\n","            detections_gdf = gpd.GeoDataFrame(all_detections, crs=\"EPSG:4326\")\n","\n","            # Show progress for duplicate removal\n","            with tqdm(total=1, desc=\"Removing duplicates\", leave=True) as pbar:\n","                detections_gdf = remove_duplicates(detections_gdf)\n","                pbar.update(1)\n","\n","        # Save results\n","        detections_gdf.to_file(paths['output_path'], driver='GeoJSON')\n","\n","        # Final summary - just two lines\n","        minutes = (time.time() - start_time) / 60\n","        print(f\"\\nCompleted in {minutes:.1f} min. Found {len(detections_gdf)} objects.\")\n","\n","        return detections_gdf\n","\n","    except Exception as e:\n","        print(f\"\\nError occurred. Check detection_debug.log for details.\")\n","        logging.error(f\"Processing failed: {str(e)}\")\n","        return gpd.GeoDataFrame(\n","            columns=['geometry', 'confidence', 'class_id', 'class_score', 'bbox'],\n","            crs=\"EPSG:4326\"\n","        )\n","\n","if __name__ == \"__main__\":\n","    detections_gdf = main()\n","\n","    # Print summary\n","    print(f\"\\nProcessing Summary:\")\n","    # print(f\"Total tiles processed: {len(tiles_gdf)}\")\n","    print(f\"Total detections: {len(detections_gdf)}\")\n","    if len(detections_gdf) > 0:\n","        print(f\"Average confidence: {detections_gdf['confidence'].mean():.3f}\")"],"metadata":{"id":"yN3V7R8AZnO3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GPU 1st attempt"],"metadata":{"id":"uAoHJ9C4CdE-"}},{"cell_type":"code","source":["!pip uninstall -y onnxruntime\n","!pip install onnxruntime-gpu"],"metadata":{"id":"f4jI7fs5G3f4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnxruntime as ort\n","print(\"Available Providers:\", ort.get_available_providers())\n","print(\"ONNX Runtime Version:\", ort.__version__)"],"metadata":{"id":"Ra01h12jG5z7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import onnxruntime as ort\n","import numpy as np\n","import cv2\n","from owslib.wms import WebMapService\n","import geopandas as gpd\n","from shapely.geometry import Point\n","import io\n","from PIL import Image\n","import os\n","from google.colab import drive\n","import torch\n","import concurrent.futures\n","import requests\n","from tqdm.auto import tqdm\n","import time\n","import gc\n","import math\n","from pyproj import Transformer\n","from shapely.ops import transform\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","class CarDetector:\n","    def __init__(self):\n","        print(\"Initializing detector...\")\n","        drive.mount('/content/drive', force_remount=True)\n","        self.base_dir = \"/content/drive/Othercomputers/My_laptop/car_recognition\"\n","\n","        # Paths\n","        self.model_path = os.path.join(self.base_dir, \"models/car_aerial_detection_yolo7_ITCVD_deepness.onnx\")\n","        self.frame_path = os.path.join(self.base_dir, \"gis/shp/frames/warsaw.shp\")\n","        self.output_path = os.path.join(self.base_dir, \"gis/shp/detection_results/warsaw.geojson\")\n","        self.checkpoint_dir = os.path.join(self.base_dir, \"checkpoints\")\n","        os.makedirs(self.checkpoint_dir, exist_ok=True)\n","\n","        # Configuration\n","        self.config = {\n","            'wms_url': \"https://mapy.geoportal.gov.pl/wss/service/PZGIK/ORTO/WMS/StandardResolution\",\n","            'tile_size_meters': 64.0,\n","            'pixel_size': 0.10,\n","            'confidence_threshold': 0.4,\n","            'tile_overlap': 0.2,\n","            'batch_size': 512,  # Increased for memory utilization\n","            'checkpoint_interval': 1000,\n","            'num_workers': 32,  # Increased for parallel fetching\n","            'queue_size': 512   # GPU memory queue size\n","        }\n","\n","        self._setup_components()\n","\n","    def _setup_components(self):\n","        \"\"\"Initialize all components\"\"\"\n","        self._setup_gpu()\n","        self._setup_model()\n","        self._setup_wms()\n","\n","    def _setup_gpu(self):\n","        \"\"\"Configure GPU and CUDA\"\"\"\n","        if not torch.cuda.is_available():\n","            raise RuntimeError(\"CUDA is not available!\")\n","\n","        torch.cuda.empty_cache()\n","        torch.backends.cudnn.benchmark = True\n","        device = torch.cuda.current_device()\n","        print(f\"\\nGPU Info:\")\n","        print(f\"Device: {torch.cuda.get_device_name(device)}\")\n","        print(f\"Memory: {torch.cuda.get_device_properties(device).total_memory/1e9:.1f}GB\")\n","\n","    def _setup_model(self):\n","        \"\"\"Initialize ONNX model with GPU optimization\"\"\"\n","        provider_options = {\n","            'device_id': 0,\n","            'gpu_mem_limit': int(14 * 1024 * 1024 * 1024),  # 14GB limit\n","            'arena_extend_strategy': 'kNextPowerOfTwo',\n","            'cudnn_conv_algo_search': 'EXHAUSTIVE'\n","        }\n","\n","        providers = [('CUDAExecutionProvider', provider_options)]\n","\n","        sess_options = ort.SessionOptions()\n","        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n","        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\n","        sess_options.intra_op_num_threads = 4\n","        sess_options.inter_op_num_threads = 4\n","\n","        self.session = ort.InferenceSession(\n","            self.model_path,\n","            sess_options=sess_options,\n","            providers=providers\n","        )\n","        print(\"Model loaded with GPU optimization\")\n","\n","    def _setup_wms(self):\n","        \"\"\"Initialize WMS connection\"\"\"\n","        self.wms = WebMapService(self.config['wms_url'], version='1.3.0')\n","        print(\"WMS connection established\")\n","\n","    def get_wms_image(self, bbox):\n","        \"\"\"Fetch single WMS image\"\"\"\n","        try:\n","            img = self.wms.getmap(\n","                layers=['Raster'],\n","                srs='EPSG:4326',\n","                bbox=bbox,\n","                size=(640, 640),\n","                format='image/jpeg',\n","                transparent=False\n","            )\n","            return Image.open(io.BytesIO(img.read())).convert('RGB')\n","        except Exception:\n","            return None\n","\n","    def fetch_images_parallel(self, tile_bboxes):\n","        \"\"\"Fetch multiple images in parallel\"\"\"\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config['num_workers']) as executor:\n","            futures = [executor.submit(self.get_wms_image, bbox) for bbox in tile_bboxes]\n","            results = []\n","            for future, bbox in zip(futures, tile_bboxes):\n","                try:\n","                    img = future.result()\n","                    if img is not None:\n","                        results.append((img, bbox))\n","                except Exception:\n","                    continue\n","            return results\n","\n","    def preprocess_image(self, img):\n","        \"\"\"Preprocess image and move to GPU\"\"\"\n","        img_array = np.array(img)\n","        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n","        tensor = torch.from_numpy(img_bgr).cuda()\n","        tensor = tensor.permute(2, 0, 1).float() / 255.0\n","        return tensor\n","\n","    def process_gpu_queue(self, gpu_queue):\n","        \"\"\"Process a queue of images stored in GPU memory\"\"\"\n","        detections = []\n","\n","        for tensor, bbox in gpu_queue:\n","            try:\n","                # Prepare for model (add batch dimension)\n","                input_tensor = tensor.unsqueeze(0).cpu().numpy()\n","\n","                # Run inference\n","                outputs = self.session.run(None, {self.session.get_inputs()[0].name: input_tensor})\n","\n","                # Process detections\n","                boxes = outputs[0][0]\n","                conf_mask = boxes[:, 4] > self.config['confidence_threshold']\n","                boxes = boxes[conf_mask]\n","\n","                if len(boxes) > 0:\n","                    # Move boxes to GPU for faster processing\n","                    boxes_tensor = torch.from_numpy(boxes).cuda()\n","                    centers = boxes_tensor[:, :2] / 640\n","\n","                    # Calculate coordinates\n","                    lon_offset = bbox[2] - bbox[0]\n","                    lat_offset = bbox[3] - bbox[1]\n","\n","                    lons = bbox[0] + (centers[:, 0] * lon_offset)\n","                    lats = bbox[3] - (centers[:, 1] * lat_offset)\n","                    confs = boxes_tensor[:, 4]\n","\n","                    # Create detections\n","                    for lon, lat, conf in zip(lons.cpu().numpy(),\n","                                            lats.cpu().numpy(),\n","                                            confs.cpu().numpy()):\n","                        detections.append({\n","                            'geometry': Point(lon, lat),\n","                            'confidence': float(conf)\n","                        })\n","\n","                    del boxes_tensor\n","\n","            except Exception as e:\n","                print(f\"Error processing queue item: {str(e)}\")\n","                continue\n","\n","        return detections\n","\n","    def process_tile_batch(self, image_bbox_pairs):\n","        \"\"\"Process a batch of images using GPU memory queue\"\"\"\n","        if not image_bbox_pairs:\n","            return []\n","\n","        all_detections = []\n","        gpu_queue = []\n","\n","        try:\n","            print(f\"\\nPreprocessing batch of {len(image_bbox_pairs)} images...\")\n","\n","            # Fill GPU queue\n","            for img, bbox in image_bbox_pairs:\n","                try:\n","                    tensor = self.preprocess_image(img)\n","                    gpu_queue.append((tensor, bbox))\n","\n","                    # Process queue if full\n","                    if len(gpu_queue) >= self.config['queue_size']:\n","                        print(f\"\\nProcessing queue of {len(gpu_queue)} images...\")\n","                        allocated = torch.cuda.memory_allocated() / 1e9\n","                        reserved = torch.cuda.memory_reserved() / 1e9\n","                        print(f\"GPU Memory - Allocated: {allocated:.1f}GB, Reserved: {reserved:.1f}GB\")\n","\n","                        # Process current queue\n","                        queue_detections = self.process_gpu_queue(gpu_queue)\n","                        all_detections.extend(queue_detections)\n","\n","                        # Clear queue\n","                        del gpu_queue[:]\n","                        gpu_queue = []\n","                        torch.cuda.empty_cache()\n","\n","                except Exception as e:\n","                    print(f\"Error preprocessing image: {str(e)}\")\n","                    continue\n","\n","            # Process remaining images\n","            if gpu_queue:\n","                print(f\"\\nProcessing remaining {len(gpu_queue)} images...\")\n","                queue_detections = self.process_gpu_queue(gpu_queue)\n","                all_detections.extend(queue_detections)\n","\n","            return all_detections\n","\n","        except Exception as e:\n","            print(f\"Batch processing error: {str(e)}\")\n","            return []\n","\n","        finally:\n","            # Clean up\n","            if 'gpu_queue' in locals():\n","                del gpu_queue\n","            torch.cuda.empty_cache()\n","\n","    def generate_tiles(self, bounds):\n","        \"\"\"Generate tile coordinates\"\"\"\n","        minx, miny, maxx, maxy = bounds\n","        mid_lat = (miny + maxy) / 2\n","\n","        # Convert meters to degrees\n","        earth_radius = 6378137\n","        tile_meters = self.config['tile_size_meters']\n","        lat_deg = tile_meters / (earth_radius * math.pi / 180)\n","        lon_deg = tile_meters / (earth_radius * math.pi / 180 * math.cos(math.radians(mid_lat)))\n","\n","        overlap = self.config['tile_overlap']\n","        step_lon = lon_deg * (1 - overlap)\n","        step_lat = lat_deg * (1 - overlap)\n","\n","        tiles = []\n","        x = minx\n","        while x < maxx:\n","            y = miny\n","            while y < maxy:\n","                tiles.append((\n","                    x, y,\n","                    min(x + lon_deg, maxx),\n","                    min(y + lat_deg, maxy)\n","                ))\n","                y += step_lat\n","            x += step_lon\n","\n","        return tiles\n","\n","    def save_checkpoint(self, detections, checkpoint_num, processed_tiles):\n","        \"\"\"Save detection results checkpoint\"\"\"\n","        if detections:\n","            checkpoint_path = os.path.join(\n","                self.checkpoint_dir,\n","                f\"checkpoint_{checkpoint_num}_tiles_{processed_tiles}.geojson\"\n","            )\n","            gdf = gpd.GeoDataFrame(detections, crs=\"EPSG:4326\")\n","            gdf.to_file(checkpoint_path, driver='GeoJSON')\n","            print(f\"\\nSaved checkpoint {checkpoint_num} with {len(detections)} detections\")\n","\n","    def remove_duplicates(self, gdf):\n","        \"\"\"Remove duplicate detections using spatial index\"\"\"\n","        if len(gdf) == 0:\n","            return gdf\n","\n","        # Convert to UTM for accurate distance calculation\n","        transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32633\", always_xy=True)\n","        gdf['geometry'] = gdf['geometry'].apply(lambda p: transform(transformer.transform, p))\n","\n","        # Sort by confidence\n","        gdf = gdf.sort_values('confidence', ascending=False)\n","\n","        # Remove duplicates\n","        kept_indices = []\n","        for idx in gdf.index:\n","            if idx in kept_indices:\n","                continue\n","\n","            point = gdf.loc[idx, 'geometry']\n","            distances = gdf['geometry'].apply(lambda p: point.distance(p))\n","            duplicates = distances[distances < 2.0].index\n","            kept_indices.extend(duplicates)\n","\n","        # Convert back to WGS84\n","        transformer = Transformer.from_crs(\"EPSG:32633\", \"EPSG:4326\", always_xy=True)\n","        filtered_gdf = gdf.loc[kept_indices]\n","        filtered_gdf['geometry'] = filtered_gdf['geometry'].apply(\n","            lambda p: transform(transformer.transform, p)\n","        )\n","\n","        return filtered_gdf\n","\n","    def detect(self):\n","        \"\"\"Main detection process with high GPU memory utilization\"\"\"\n","        try:\n","            # Load frame\n","            print(\"\\nLoading frame...\")\n","            frame_gdf = gpd.read_file(self.frame_path)\n","            if frame_gdf.crs.to_epsg() != 4326:\n","                frame_gdf = frame_gdf.to_crs(epsg=4326)\n","\n","            bounds = frame_gdf.total_bounds\n","\n","            # Generate tiles\n","            print(\"Generating tiles...\")\n","            tiles = self.generate_tiles(bounds)\n","            total_tiles = len(tiles)\n","            print(f\"Generated {total_tiles} tiles\")\n","\n","            # Initialize processing\n","            all_detections = []\n","            checkpoint_num = 0\n","            processed_count = 0\n","\n","            # Process tiles in batches\n","            progress_bar = tqdm(total=total_tiles, desc=\"Processing tiles\", unit=\"tiles\")\n","\n","            for start_idx in range(0, total_tiles, self.config['batch_size']):\n","                try:\n","                    # Get current batch\n","                    end_idx = min(start_idx + self.config['batch_size'], total_tiles)\n","                    current_batch = tiles[start_idx:end_idx]\n","\n","                    # Fetch and process images\n","                    print(f\"\\nFetching batch of {len(current_batch)} images...\")\n","                    image_bbox_pairs = self.fetch_images_parallel(current_batch)\n","\n","                    if image_bbox_pairs:\n","                        # Process batch\n","                        batch_detections = self.process_tile_batch(image_bbox_pairs)\n","                        all_detections.extend(batch_detections)\n","\n","                        # Update progress\n","                        batch_processed = len(current_batch)\n","                        processed_count += batch_processed\n","                        progress_bar.update(batch_processed)\n","\n","                        # Print detailed status\n","                        print(f\"\\nBatch Statistics:\")\n","                        print(f\"Processed tiles: {processed_count}/{total_tiles}\")\n","                        print(f\"Current detections: {len(all_detections)}\")\n","                        print(f\"Batch detections: {len(batch_detections)}\")\n","\n","                        allocated = torch.cuda.memory_allocated() / 1e9\n","                        reserved = torch.cuda.memory_reserved() / 1e9\n","                        print(f\"GPU Memory - Allocated: {allocated:.1f}GB, Reserved: {reserved:.1f}GB\")\n","\n","                        # Save checkpoint\n","                        if processed_count % self.config['checkpoint_interval'] == 0:\n","                            checkpoint_num += 1\n","                            self.save_checkpoint(all_detections, checkpoint_num, processed_count)\n","\n","                    # Clean up memory\n","                    gc.collect()\n","                    torch.cuda.empty_cache()\n","                except Exception as e:\n","                    print(f\"\\nError processing batch: {str(e)}\")\n","                    continue\n","\n","            progress_bar.close()\n","\n","            # Process final results\n","            if all_detections:\n","                print(\"\\nProcessing final results...\")\n","                detections_gdf = gpd.GeoDataFrame(all_detections, crs=\"EPSG:4326\")\n","                print(f\"Raw detections: {len(detections_gdf)}\")\n","\n","                print(\"Removing duplicates...\")\n","                detections_gdf = self.remove_duplicates(detections_gdf)\n","                print(f\"Final detections: {len(detections_gdf)}\")\n","\n","                detections_gdf.to_file(self.output_path, driver='GeoJSON')\n","                return detections_gdf\n","\n","            return gpd.GeoDataFrame(columns=['geometry', 'confidence'], crs=\"EPSG:4326\")\n","\n","        except Exception as e:\n","            print(f\"Error in detection process: {str(e)}\")\n","            import traceback\n","            traceback.print_exc()\n","            return gpd.GeoDataFrame(columns=['geometry', 'confidence'], crs=\"EPSG:4326\")\n","\n","def main():\n","    try:\n","        # Initialize detector\n","        detector = CarDetector()\n","\n","        # Run detection\n","        print(\"\\nStarting detection process...\")\n","        start_time = time.time()\n","\n","        results = detector.detect()\n","\n","        # Print final statistics\n","        elapsed_time = (time.time() - start_time) / 60  # Convert to minutes\n","        print(\"\\nProcessing Complete!\")\n","        print(f\"Total time: {elapsed_time:.2f} minutes\")\n","\n","        if len(results) > 0:\n","            print(f\"\\nFinal Results:\")\n","            print(f\"Total detections: {len(results)}\")\n","            print(f\"Average confidence: {results['confidence'].mean():.3f}\")\n","            print(f\"Results saved to: {detector.output_path}\")\n","\n","            # Memory usage summary\n","            allocated = torch.cuda.memory_allocated() / 1e9\n","            reserved = torch.cuda.memory_reserved() / 1e9\n","            print(f\"\\nFinal GPU Memory Status:\")\n","            print(f\"Allocated: {allocated:.1f}GB\")\n","            print(f\"Reserved: {reserved:.1f}GB\")\n","\n","            return results\n","        else:\n","            print(\"No detections found\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"Error in main process: {str(e)}\")\n","        traceback.print_exc()\n","        return None\n","    finally:\n","        # Final cleanup\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"YaatrKUgCeeu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"0QaWPh7pMlT5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7umBw8R2MlRJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## GPU 2nd attempt"],"metadata":{"id":"DbpdnWkr25gd"}},{"cell_type":"code","source":["!pip install onnxruntime-gpu owslib geopandas shapely pyproj tqdm pillow opencv-python-headless requests\n","!pip install psutil"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSsAwjiUMlO1","executionInfo":{"status":"ok","timestamp":1730321950647,"user_tz":-60,"elapsed":20901,"user":{"displayName":"Jacek Gęborys","userId":"15708106607875074455"}},"outputId":"0779a86c-06e8-4f45-f014-c4706b25d8b7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting onnxruntime-gpu\n","  Downloading onnxruntime_gpu-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting owslib\n","  Downloading OWSLib-0.32.0-py2.py3-none-any.whl.metadata (6.6 kB)\n","Requirement already satisfied: geopandas in /usr/local/lib/python3.10/dist-packages (1.0.1)\n","Requirement already satisfied: shapely in /usr/local/lib/python3.10/dist-packages (2.0.6)\n","Requirement already satisfied: pyproj in /usr/local/lib/python3.10/dist-packages (3.7.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.5)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n","Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.10/dist-packages (4.10.0.84)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n","Collecting coloredlogs (from onnxruntime-gpu)\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.3.25)\n","Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.26.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (24.1)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (3.20.3)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from onnxruntime-gpu) (1.13.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from owslib) (4.9.4)\n","Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from owslib) (2.8.2)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from owslib) (6.0.2)\n","Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from geopandas) (0.10.0)\n","Requirement already satisfied: pandas>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from geopandas) (2.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from pyproj) (2024.8.30)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4.0->geopandas) (2024.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil->owslib) (1.16.0)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime-gpu)\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->onnxruntime-gpu) (1.3.0)\n","Downloading onnxruntime_gpu-1.19.2-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (226.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.2/226.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading OWSLib-0.32.0-py2.py3-none-any.whl (240 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m240.0/240.0 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: humanfriendly, owslib, coloredlogs, onnxruntime-gpu\n","Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-gpu-1.19.2 owslib-0.32.0\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (5.9.5)\n"]}]},{"cell_type":"code","source":["import onnxruntime as ort\n","import numpy as np\n","import cv2\n","from owslib.wms import WebMapService\n","import geopandas as gpd\n","from shapely.geometry import Point\n","import io\n","from PIL import Image\n","import os\n","from google.colab import drive\n","import torch\n","import concurrent.futures\n","import requests\n","from tqdm.auto import tqdm\n","import time\n","import gc\n","import math\n","import json\n","import psutil\n","from pyproj import Transformer\n","from shapely.ops import transform\n","import warnings\n","import traceback\n","warnings.filterwarnings('ignore')\n","\n","class CarDetector:\n","    def __init__(self):\n","        print(\"Initializing detector...\")\n","        drive.mount('/content/drive', force_remount=True)\n","        self.base_dir = \"/content/drive/Othercomputers/My_laptop/car_recognition\"\n","\n","        # Create directory structure\n","        self.setup_directories()\n","\n","        # Paths\n","        self.model_path = os.path.join(self.base_dir, \"models/car_aerial_detection_yolo7_ITCVD_deepness.onnx\")\n","        self.frame_path = os.path.join(self.base_dir, \"gis/shp/frames/warsaw.shp\")\n","        self.output_path = os.path.join(self.base_dir, \"gis/shp/detection_results/warsaw.geojson\")\n","\n","        self.config = {\n","            'wms_url': \"https://mapy.geoportal.gov.pl/wss/service/PZGIK/ORTO/WMS/StandardResolution\",\n","            'tile_size_meters': 64.0,\n","            'confidence_threshold': 0.4,\n","            'tile_overlap': 0.1,\n","            'batch_size': 4096,          # Increased for faster processing\n","            'checkpoint_interval': 5000,\n","            'num_workers': 32,\n","            'queue_size': 1024,          # Increased for better GPU utilization\n","            'max_gpu_memory': 12.0,\n","            'min_gpu_memory': 8.0,\n","            'ram_chunk_size': 512,       # Increased for better RAM utilization\n","            'duplicate_distance': 2.0     # Added missing parameter (meters)\n","        }\n","\n","        # Initialize checkpoint files\n","        self.initialize_checkpoints()\n","\n","        print(f\"\\nConfiguration:\")\n","        print(f\"- GPU target memory: {self.config['min_gpu_memory']}-{self.config['max_gpu_memory']}GB\")\n","        print(f\"- Batch size: {self.config['batch_size']}\")\n","        print(f\"- Queue size: {self.config['queue_size']}\")\n","        print(f\"- Workers: {self.config['num_workers']}\")\n","        print(f\"- Checkpoint interval: {self.config['checkpoint_interval']} tiles\")\n","\n","        self._setup_components()\n","\n","    def setup_directories(self):\n","        \"\"\"Create necessary directories and verify their existence\"\"\"\n","        # Create main directories\n","        directories = [\n","            os.path.join(self.base_dir, \"checkpoints\"),\n","            os.path.join(self.base_dir, \"models\"),\n","            os.path.join(self.base_dir, \"gis/shp/detection_results\"),\n","        ]\n","\n","        for directory in directories:\n","            os.makedirs(directory, exist_ok=True)\n","            if os.path.exists(directory):\n","                print(f\"Verified directory: {directory}\")\n","            else:\n","                raise RuntimeError(f\"Failed to create directory: {directory}\")\n","\n","        self.checkpoint_dir = directories[0]\n","\n","        # Set checkpoint paths\n","        self.checkpoint_state = os.path.join(self.checkpoint_dir, \"processing_state.json\")\n","        self.checkpoint_data = os.path.join(self.checkpoint_dir, \"latest_detections.geojson\")\n","\n","    def initialize_checkpoints(self):\n","        \"\"\"Initialize checkpoint files only if they don't exist\"\"\"\n","        try:\n","            # Only create checkpoint files if they don't exist\n","            if not os.path.exists(self.checkpoint_state):\n","                # Create empty checkpoint state\n","                initial_state = {\n","                    'initialized': time.time(),\n","                    'processed_tiles': 0,\n","                    'total_tiles': 0,\n","                    'last_processed_index': -1\n","                }\n","\n","                with open(self.checkpoint_state, 'w') as f:\n","                    json.dump(initial_state, f)\n","\n","            if not os.path.exists(self.checkpoint_data):\n","                # Create empty GeoDataFrame for detections\n","                empty_gdf = gpd.GeoDataFrame(columns=['geometry', 'confidence'], crs=\"EPSG:4326\")\n","                empty_gdf.to_file(self.checkpoint_data, driver='GeoJSON')\n","\n","            # Verify files exist\n","            if os.path.exists(self.checkpoint_state) and os.path.exists(self.checkpoint_data):\n","                print(f\"\\nCheckpoint files verified:\")\n","                print(f\"- State: {self.checkpoint_state}\")\n","                print(f\"- Data: {self.checkpoint_data}\")\n","            else:\n","                raise RuntimeError(\"Failed to verify checkpoint files\")\n","\n","        except Exception as e:\n","            print(f\"Error initializing checkpoints: {str(e)}\")\n","            raise\n","\n","    def _setup_components(self):\n","        \"\"\"Initialize processing components\"\"\"\n","        self._setup_gpu()\n","        self._setup_model()\n","        self._setup_wms()\n","\n","    def _setup_gpu(self):\n","        \"\"\"Configure GPU and CUDA\"\"\"\n","        if not torch.cuda.is_available():\n","            raise RuntimeError(\"CUDA is not available!\")\n","\n","        torch.cuda.empty_cache()\n","        torch.backends.cudnn.benchmark = True\n","        torch.backends.cudnn.fastest = True\n","        device = torch.cuda.current_device()\n","\n","        print(f\"\\nGPU Information:\")\n","        print(f\"Device: {torch.cuda.get_device_name(device)}\")\n","        print(f\"Memory: {torch.cuda.get_device_properties(device).total_memory/1e9:.1f}GB\")\n","\n","        # Warm up GPU\n","        dummy = torch.zeros(1, 3, 640, 640, device='cuda')\n","        del dummy\n","        torch.cuda.empty_cache()\n","\n","    def _setup_model(self):\n","        \"\"\"Initialize ONNX model with GPU optimization\"\"\"\n","        provider_options = {\n","            'device_id': 0,\n","            'gpu_mem_limit': int(self.config['max_gpu_memory'] * 1024 * 1024 * 1024),\n","            'arena_extend_strategy': 'kNextPowerOfTwo',\n","            'cudnn_conv_algo_search': 'EXHAUSTIVE',\n","            'do_copy_in_default_stream': True\n","        }\n","\n","        providers = [('CUDAExecutionProvider', provider_options)]\n","\n","        sess_options = ort.SessionOptions()\n","        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n","        sess_options.execution_mode = ort.ExecutionMode.ORT_PARALLEL\n","        sess_options.intra_op_num_threads = 4\n","        sess_options.inter_op_num_threads = 4\n","\n","        self.session = ort.InferenceSession(\n","            self.model_path,\n","            sess_options=sess_options,\n","            providers=providers\n","        )\n","        print(\"Model loaded with GPU optimization\")\n","\n","    def _setup_wms(self):\n","        \"\"\"Initialize WMS connection\"\"\"\n","        self.wms = WebMapService(self.config['wms_url'], version='1.3.0')\n","        print(\"WMS connection established\")\n","\n","    def get_wms_image(self, bbox):\n","        \"\"\"Fetch single WMS image with timeout\"\"\"\n","        try:\n","            img = self.wms.getmap(\n","                layers=['Raster'],\n","                srs='EPSG:4326',\n","                bbox=bbox,\n","                size=(640, 640),\n","                format='image/jpeg',\n","                transparent=False,\n","                timeout=30  # Add timeout\n","            )\n","            return Image.open(io.BytesIO(img.read())).convert('RGB')\n","        except Exception:\n","            return None\n","\n","    def fetch_images_parallel(self, tile_bboxes):\n","        \"\"\"Fetch images with parallel processing and timeout\"\"\"\n","        results = []\n","        futures = []\n","\n","        with concurrent.futures.ThreadPoolExecutor(max_workers=self.config['num_workers']) as executor:\n","            # Submit all tasks\n","            for bbox in tile_bboxes:\n","                futures.append((executor.submit(self.get_wms_image, bbox), bbox))\n","\n","            # Process completed tasks\n","            for future, bbox in futures:\n","                try:\n","                    img = future.result(timeout=30)  # Add timeout for each task\n","                    if img is not None:\n","                        results.append((img, bbox))\n","                except (concurrent.futures.TimeoutError, Exception) as e:\n","                    continue\n","\n","        return results\n","\n","    def preprocess_image(self, img):\n","        \"\"\"Preprocess image and move to GPU\"\"\"\n","        img_array = np.array(img)\n","        img_bgr = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)\n","        tensor = torch.from_numpy(img_bgr).cuda()\n","        tensor = tensor.to(dtype=torch.float32)\n","        tensor = tensor.permute(2, 0, 1) / 255.0\n","        return tensor\n","\n","    def process_gpu_queue(self, gpu_queue):\n","        \"\"\"Process queue of images in GPU memory\"\"\"\n","        detections = []\n","\n","        for tensor, bbox in gpu_queue:\n","            try:\n","                # Add batch dimension and run inference\n","                input_tensor = tensor.unsqueeze(0).cpu().numpy()\n","                outputs = self.session.run(None, {self.session.get_inputs()[0].name: input_tensor})\n","\n","                boxes = outputs[0][0]\n","                conf_mask = boxes[:, 4] > self.config['confidence_threshold']\n","                boxes = boxes[conf_mask]\n","\n","                if len(boxes) > 0:\n","                    # Move boxes to GPU for faster processing\n","                    boxes_tensor = torch.from_numpy(boxes).cuda()\n","                    centers = boxes_tensor[:, :2] / 640\n","\n","                    lon_offset = bbox[2] - bbox[0]\n","                    lat_offset = bbox[3] - bbox[1]\n","\n","                    lons = bbox[0] + (centers[:, 0] * lon_offset)\n","                    lats = bbox[3] - (centers[:, 1] * lat_offset)\n","                    confs = boxes_tensor[:, 4]\n","\n","                    for lon, lat, conf in zip(lons.cpu().numpy(),\n","                                            lats.cpu().numpy(),\n","                                            confs.cpu().numpy()):\n","                        detections.append({\n","                            'geometry': Point(lon, lat),\n","                            'confidence': float(conf)\n","                        })\n","\n","                    del boxes_tensor\n","\n","            except Exception:\n","                continue\n","\n","        return detections\n","\n","    def process_tile_batch(self, image_bbox_pairs):\n","        \"\"\"Process batch with GPU optimization\"\"\"\n","        if not image_bbox_pairs:\n","            return []\n","\n","        all_detections = []\n","        try:\n","            # Convert and move all images to GPU first\n","            gpu_tensors = []\n","            for img, bbox in image_bbox_pairs:\n","                try:\n","                    tensor = self.preprocess_image(img)\n","                    gpu_tensors.append((tensor, bbox))\n","                except Exception:\n","                    continue\n","\n","            # Process in smaller sub-batches to maintain GPU memory\n","            for i in range(0, len(gpu_tensors), self.config['queue_size']):\n","                sub_batch = gpu_tensors[i:i + self.config['queue_size']]\n","                batch_detections = self.process_gpu_queue(sub_batch)\n","                all_detections.extend(batch_detections)\n","\n","            return all_detections\n","\n","        except Exception as e:\n","            print(f\"Batch processing error: {str(e)}\")\n","            return []\n","        finally:\n","            # Clean up GPU memory\n","            if 'gpu_tensors' in locals():\n","                del gpu_tensors\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","    def remove_duplicates(self, detections):\n","        \"\"\"Remove duplicate detections efficiently\"\"\"\n","        if not detections:\n","            return []\n","\n","        try:\n","            gdf = gpd.GeoDataFrame(detections, crs=\"EPSG:4326\")\n","\n","            # Convert to UTM for accurate distance calculation\n","            transformer = Transformer.from_crs(\"EPSG:4326\", \"EPSG:32633\", always_xy=True)\n","            gdf['geometry'] = gdf['geometry'].apply(lambda p: transform(transformer.transform, p))\n","\n","            # Sort by confidence\n","            gdf = gdf.sort_values('confidence', ascending=False)\n","\n","            # Remove duplicates\n","            kept_indices = []\n","            for idx in gdf.index:\n","                if idx in kept_indices:\n","                    continue\n","\n","                point = gdf.loc[idx, 'geometry']\n","                distances = gdf['geometry'].apply(lambda p: point.distance(p))\n","                duplicates = distances[distances < self.config['duplicate_distance']].index\n","                kept_indices.extend(duplicates)\n","\n","            # Convert back to WGS84\n","            transformer = Transformer.from_crs(\"EPSG:32633\", \"EPSG:4326\", always_xy=True)\n","            filtered_gdf = gdf.loc[kept_indices]\n","            filtered_gdf['geometry'] = filtered_gdf['geometry'].apply(\n","                lambda p: transform(transformer.transform, p)\n","            )\n","\n","            return filtered_gdf.to_dict('records')\n","\n","        except Exception as e:\n","            print(f\"Deduplication error: {str(e)}\")\n","            return detections  # Return original if deduplication fails\n","\n","    def save_checkpoint(self, detections, processed_tiles, total_tiles):\n","        \"\"\"Save checkpoint with processing state\"\"\"\n","        try:\n","            # Save processing state\n","            checkpoint_data = {\n","                'processed_tiles': processed_tiles,\n","                'total_tiles': total_tiles,\n","                'last_processed_index': processed_tiles - 1,\n","                'timestamp': time.time()\n","            }\n","\n","            with open(self.checkpoint_state, 'w') as f:\n","                json.dump(checkpoint_data, f)\n","\n","            # Save detections\n","            unique_detections = self.remove_duplicates(detections)\n","            gdf = gpd.GeoDataFrame(unique_detections, crs=\"EPSG:4326\")\n","            gdf.to_file(self.checkpoint_data, driver='GeoJSON')\n","\n","            print(f\"\\nCheckpoint saved at {processed_tiles}/{total_tiles} tiles\")\n","            print(f\"Stored {len(unique_detections)} unique detections\")\n","\n","        except Exception as e:\n","            print(f\"Checkpoint save error: {str(e)}\")\n","\n","    def load_checkpoint(self):\n","        \"\"\"Load previous checkpoint if exists\"\"\"\n","        if os.path.exists(self.checkpoint_state) and os.path.exists(self.checkpoint_data):\n","            try:\n","                with open(self.checkpoint_state, 'r') as f:\n","                    checkpoint_data = json.load(f)\n","\n","                gdf = gpd.read_file(self.checkpoint_data)\n","                detections = gdf.to_dict('records')\n","\n","                print(f\"\\nFound checkpoint:\")\n","                print(f\"- Processed: {checkpoint_data['processed_tiles']} tiles\")\n","                print(f\"- Detections: {len(detections)}\")\n","                print(f\"- Time: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(checkpoint_data['timestamp']))}\")\n","\n","                response = input(\"\\nResume from checkpoint? (y/n): \").lower()\n","                if response == 'y':\n","                    return checkpoint_data, detections\n","                else:\n","                    print(\"Starting fresh processing...\")\n","                    os.remove(self.checkpoint_state)\n","                    os.remove(self.checkpoint_data)\n","\n","            except Exception as e:\n","                print(f\"Error loading checkpoint: {str(e)}\")\n","\n","        return None, []\n","\n","    def generate_tiles(self, bounds):\n","        \"\"\"Generate tile coordinates\"\"\"\n","        minx, miny, maxx, maxy = bounds\n","        mid_lat = (miny + maxy) / 2\n","\n","        # Convert meters to degrees\n","        earth_radius = 6378137\n","        tile_meters = self.config['tile_size_meters']\n","        lat_deg = tile_meters / (earth_radius * math.pi / 180)\n","        lon_deg = tile_meters / (earth_radius * math.pi / 180 * math.cos(math.radians(mid_lat)))\n","\n","        overlap = self.config['tile_overlap']\n","        step_lon = lon_deg * (1 - overlap)\n","        step_lat = lat_deg * (1 - overlap)\n","\n","        tiles = []\n","        x = minx\n","        while x < maxx:\n","            y = miny\n","            while y < maxy:\n","                tiles.append((\n","                    x, y,\n","                    min(x + lon_deg, maxx),\n","                    min(y + lat_deg, maxy)\n","                ))\n","                y += step_lat\n","            x += step_lon\n","\n","        return tiles\n","\n","    def detect(self):\n","        \"\"\"Main detection process with optimized performance\"\"\"\n","        try:\n","            start_time = time.time()\n","\n","            # Load frame\n","            frame_gdf = gpd.read_file(self.frame_path)\n","            if frame_gdf.crs.to_epsg() != 4326:\n","                frame_gdf = frame_gdf.to_crs(epsg=4326)\n","\n","            # Generate tiles\n","            tiles = self.generate_tiles(frame_gdf.total_bounds)\n","            total_tiles = len(tiles)\n","            del frame_gdf\n","            gc.collect()\n","\n","            # Load checkpoint or initialize\n","            checkpoint_data, all_detections = self.load_checkpoint()\n","            if checkpoint_data:\n","                processed_count = checkpoint_data['processed_tiles']\n","                start_idx = checkpoint_data['last_processed_index'] + 1\n","            else:\n","                processed_count = 0\n","                start_idx = 0\n","                all_detections = []\n","\n","            print(f\"\\nProcessing {total_tiles} tiles...\")\n","            progress_bar = tqdm(total=total_tiles, initial=processed_count, desc=\"Processing\", unit=\"tiles\")\n","\n","            last_checkpoint = processed_count\n","\n","            # Process tiles in batches\n","            for idx in range(start_idx, total_tiles, self.config['batch_size']):\n","                try:\n","                    # Get current batch\n","                    end_idx = min(idx + self.config['batch_size'], total_tiles)\n","                    current_batch = tiles[idx:end_idx]\n","\n","                    # Process batch\n","                    image_bbox_pairs = self.fetch_images_parallel(current_batch)\n","\n","                    if image_bbox_pairs:\n","                        batch_detections = self.process_tile_batch(image_bbox_pairs)\n","\n","                        # Manage memory for detections\n","                        if len(all_detections) > 100000:\n","                            all_detections = self.remove_duplicates(all_detections)\n","\n","                        all_detections.extend(batch_detections)\n","\n","                        # Update progress\n","                        batch_processed = len(current_batch)\n","                        processed_count += batch_processed\n","                        progress_bar.update(batch_processed)\n","\n","                        # Check for checkpoint\n","                        if processed_count - last_checkpoint >= self.config['checkpoint_interval']:\n","                            self.save_checkpoint(all_detections, processed_count, total_tiles)\n","                            last_checkpoint = processed_count\n","\n","                        # Print status every 10k tiles\n","                        if processed_count % 10000 == 0:\n","                            elapsed = (time.time() - start_time) / 60\n","                            remaining = (elapsed / (processed_count - start_idx + 1)) * (total_tiles - processed_count)\n","                            print(f\"\\nStatus Update:\")\n","                            print(f\"Time: {elapsed:.1f}min elapsed, ~{remaining:.1f}min remaining\")\n","                            print(f\"Memory: {psutil.Process().memory_info().rss/1e9:.1f}GB RAM\")\n","                            print(f\"GPU: {torch.cuda.memory_allocated()/1e9:.1f}GB\")\n","                            print(f\"Detections: {len(batch_detections)} in batch, {len(all_detections)} total\")\n","\n","                    # Clean up\n","                    del image_bbox_pairs\n","                    gc.collect()\n","                    torch.cuda.empty_cache()\n","\n","                except Exception as e:\n","                    print(f\"\\nBatch error: {str(e)}\")\n","                    self.save_checkpoint(all_detections, processed_count, total_tiles)\n","                    continue\n","\n","            progress_bar.close()\n","\n","            # Continuing from detect() method\n","            # Process final results\n","            if all_detections:\n","                print(\"\\nProcessing final results...\")\n","                final_detections = self.remove_duplicates(all_detections)\n","                results_gdf = gpd.GeoDataFrame(final_detections, crs=\"EPSG:4326\")\n","\n","                print(f\"Saving {len(results_gdf)} final detections...\")\n","                results_gdf.to_file(self.output_path, driver='GeoJSON')\n","\n","                total_time = (time.time() - start_time) / 60\n","                print(f\"\\nProcessing Complete!\")\n","                print(f\"Time: {total_time:.1f} minutes\")\n","                print(f\"Tiles: {total_tiles}\")\n","                print(f\"Detections: {len(results_gdf)}\")\n","                print(f\"Results saved to: {self.output_path}\")\n","\n","                # Clean up checkpoint files on successful completion\n","                if os.path.exists(self.checkpoint_state):\n","                    os.remove(self.checkpoint_state)\n","                if os.path.exists(self.checkpoint_data):\n","                    os.remove(self.checkpoint_data)\n","\n","                return results_gdf\n","\n","            return gpd.GeoDataFrame(columns=['geometry', 'confidence'], crs=\"EPSG:4326\")\n","\n","        except Exception as e:\n","            print(f\"\\nError in detection process: {str(e)}\")\n","            traceback.print_exc()\n","            # Try to save checkpoint on error\n","            try:\n","                self.save_checkpoint(all_detections, processed_count, total_tiles)\n","                print(\"Checkpoint saved after error - you can resume later\")\n","            except:\n","                print(\"Failed to save checkpoint after error\")\n","            return gpd.GeoDataFrame(columns=['geometry', 'confidence'], crs=\"EPSG:4326\")\n","        finally:\n","            # Final cleanup\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","def main():\n","    \"\"\"Main execution with error handling\"\"\"\n","    try:\n","        # Initialize detector\n","        detector = CarDetector()\n","\n","        # Run detection\n","        print(\"\\nStarting detection process...\")\n","        start_time = time.time()\n","\n","        results = detector.detect()\n","\n","        if len(results) > 0:\n","            print(f\"\\nFinal Statistics:\")\n","            print(f\"- Total detections: {len(results)}\")\n","            print(f\"- Average confidence: {results['confidence'].mean():.3f}\")\n","\n","            # Memory usage summary\n","            ram_usage = psutil.Process().memory_info().rss / 1e9\n","            gpu_allocated = torch.cuda.memory_allocated() / 1e9\n","            gpu_reserved = torch.cuda.memory_reserved() / 1e9\n","\n","            print(f\"\\nFinal Memory Usage:\")\n","            print(f\"- RAM: {ram_usage:.1f}GB\")\n","            print(f\"- GPU Allocated: {gpu_allocated:.1f}GB\")\n","            print(f\"- GPU Reserved: {gpu_reserved:.1f}GB\")\n","\n","            total_time = (time.time() - start_time) / 60\n","            print(f\"\\nTotal processing time: {total_time:.1f} minutes\")\n","            print(f\"Average speed: {len(results)/total_time:.1f} detections/minute\")\n","\n","            return results\n","        else:\n","            print(\"No detections found\")\n","            return None\n","\n","    except Exception as e:\n","        print(f\"Error in main process: {str(e)}\")\n","        traceback.print_exc()\n","        return None\n","    finally:\n","        # Final cleanup\n","        torch.cuda.empty_cache()\n","        gc.collect()\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOYjJ3owMmVw","outputId":"7db31c23-bf54-4762-c9c0-318120209a98","executionInfo":{"status":"ok","timestamp":1730321974356,"user_tz":-60,"elapsed":23713,"user":{"displayName":"Jacek Gęborys","userId":"15708106607875074455"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Initializing detector...\n","Mounted at /content/drive\n","Verified directory: /content/drive/Othercomputers/My_laptop/car_recognition/checkpoints\n","Verified directory: /content/drive/Othercomputers/My_laptop/car_recognition/models\n","Verified directory: /content/drive/Othercomputers/My_laptop/car_recognition/gis/shp/detection_results\n","\n","Checkpoint files verified:\n","- State: /content/drive/Othercomputers/My_laptop/car_recognition/checkpoints/processing_state.json\n","- Data: /content/drive/Othercomputers/My_laptop/car_recognition/checkpoints/latest_detections.geojson\n","\n","Configuration:\n","- GPU target memory: 8.0-12.0GB\n","- Batch size: 4096\n","- Queue size: 1024\n","- Workers: 32\n","- Checkpoint interval: 5000 tiles\n","\n","GPU Information:\n","Device: Tesla T4\n","Memory: 15.8GB\n","Model loaded with GPU optimization\n","Error in main process: HTTPSConnectionPool(host='mapy.geoportal.gov.pl', port=443): Max retries exceeded with url: /wss/service/PZGIK/ORTO/WMS/StandardResolution?service=WMS&request=GetCapabilities&version=1.3.0 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78c0a8a8e050>: Failed to resolve 'mapy.geoportal.gov.pl' ([Errno -2] Name or service not known)\"))\n"]},{"output_type":"stream","name":"stderr","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 199, in _new_conn\n","    sock = connection.create_connection(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/connection.py\", line 60, in create_connection\n","    for res in socket.getaddrinfo(host, port, family, socket.SOCK_STREAM):\n","  File \"/usr/lib/python3.10/socket.py\", line 955, in getaddrinfo\n","    for res in _socket.getaddrinfo(host, port, family, type, proto, flags):\n","socket.gaierror: [Errno -2] Name or service not known\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 789, in urlopen\n","    response = self._make_request(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 490, in _make_request\n","    raise new_e\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 466, in _make_request\n","    self._validate_conn(conn)\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 1095, in _validate_conn\n","    conn.connect()\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 693, in connect\n","    self.sock = sock = self._new_conn()\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connection.py\", line 206, in _new_conn\n","    raise NameResolutionError(self.host, self, e) from e\n","urllib3.exceptions.NameResolutionError: <urllib3.connection.HTTPSConnection object at 0x78c0a8a8e050>: Failed to resolve 'mapy.geoportal.gov.pl' ([Errno -2] Name or service not known)\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 667, in send\n","    resp = conn.urlopen(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/connectionpool.py\", line 843, in urlopen\n","    retries = retries.increment(\n","  File \"/usr/local/lib/python3.10/dist-packages/urllib3/util/retry.py\", line 519, in increment\n","    raise MaxRetryError(_pool, url, reason) from reason  # type: ignore[arg-type]\n","urllib3.exceptions.MaxRetryError: HTTPSConnectionPool(host='mapy.geoportal.gov.pl', port=443): Max retries exceeded with url: /wss/service/PZGIK/ORTO/WMS/StandardResolution?service=WMS&request=GetCapabilities&version=1.3.0 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78c0a8a8e050>: Failed to resolve 'mapy.geoportal.gov.pl' ([Errno -2] Name or service not known)\"))\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"<ipython-input-3-c1539795a5e5>\", line 548, in main\n","    detector = CarDetector()\n","  File \"<ipython-input-3-c1539795a5e5>\", line 65, in __init__\n","    self._setup_components()\n","  File \"<ipython-input-3-c1539795a5e5>\", line 126, in _setup_components\n","    self._setup_wms()\n","  File \"<ipython-input-3-c1539795a5e5>\", line 174, in _setup_wms\n","    self.wms = WebMapService(self.config['wms_url'], version='1.3.0')\n","  File \"/usr/local/lib/python3.10/dist-packages/owslib/wms.py\", line 54, in WebMapService\n","    return wms130.WebMapService_1_3_0(\n","  File \"/usr/local/lib/python3.10/dist-packages/owslib/map/wms130.py\", line 74, in __init__\n","    self._capabilities = reader.read(self.url, timeout=self.timeout)\n","  File \"/usr/local/lib/python3.10/dist-packages/owslib/map/common.py\", line 65, in read\n","    u = openURL(spliturl[0], spliturl[1], method='Get',\n","  File \"/usr/local/lib/python3.10/dist-packages/owslib/util.py\", line 207, in openURL\n","    req = requests.request(method.upper(), url_base, headers=headers, **rkwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/api.py\", line 59, in request\n","    return session.request(method=method, url=url, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 589, in request\n","    resp = self.send(prep, **send_kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/sessions.py\", line 703, in send\n","    r = adapter.send(request, **kwargs)\n","  File \"/usr/local/lib/python3.10/dist-packages/requests/adapters.py\", line 700, in send\n","    raise ConnectionError(e, request=request)\n","requests.exceptions.ConnectionError: HTTPSConnectionPool(host='mapy.geoportal.gov.pl', port=443): Max retries exceeded with url: /wss/service/PZGIK/ORTO/WMS/StandardResolution?service=WMS&request=GetCapabilities&version=1.3.0 (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x78c0a8a8e050>: Failed to resolve 'mapy.geoportal.gov.pl' ([Errno -2] Name or service not known)\"))\n"]}]}]}